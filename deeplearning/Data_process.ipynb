{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本的数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Series` 和 `Dataframe` 是 `pandas` 库里面两个常见的数据类型，为了方便你们快速上手，我们会对它们进行相应的介绍以及进行一些简单的代码实操。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 如果你不会使用 `Jupyter Notebook`，请移步至：[Jupyter Notebook的使用](https://zhuanlan.zhihu.com/p/33105153)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed to ensure that we can replicate the numbers generated (for random function)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Series(np.random.randn(10))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use index to select specific values in s, have a try!\n",
    "# s[1:3],s[[1,2]],s[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series是一个具有轴标签的1-D numpy数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the index\n",
    "# Pandas data structure is extending numpy ndarray\n",
    "# but it comes with index(es)\n",
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index for the copy of s.\n",
    "s1 = s.copy()\n",
    "s1.index = [\"item 0\", \"item 1\", \"item 2\", \"item 3\", \"item 4\", \"item 5\", \"item 6\", \"item 7\", \"item 8\", \"item 9\"]\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reverse of s1\n",
    "s2 = Series(s.values[::-1], index=[\"item 0\", \"item 1\", \"item 2\", \"item 3\", \"item 4\", \"item 5\", \"item 6\", \"item 7\", \"item 8\", \"item 9\"])\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to perform s1 + s2\n",
    "s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice we are having same indexes\n",
    "s3 = pd.Series([\"d\", \"e\"])\n",
    "s4 = pd.Series([\"f\", \"g\"])\n",
    "s5 = pd.concat([s2, s3])\n",
    "s5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s5[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q1. 我们如何将 `s3` 和 `s4` 和合并在一起，同时保证 `index` 是递增的呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s6=pd.concat([s3,s4],ignore_index=True)\n",
    "s6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame` 是一个二维表格数据。`Series` 可用的操作也可用于 `Dataframe` (或类似的方式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame([[1,2,3,4,5], [6,7,8,9,10]], columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请下载我们为你准备的 `googleplaystore.csv` 文件，运用 `pandas` 库将其读入，并根据下面的要求进行相应的处理。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplay = pd.read_csv(\"googleplaystore.csv\")\n",
    "gplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q2. 试去除 `gplay` 中的 `Nan` 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplay.dropna(inplace=True)\n",
    "\n",
    "\n",
    "gplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q3. 查看`gplay`中重复的列，试使用`iloc`和`loc`来选中重复的行，并谈谈他们的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also possible to use the duplicated() method \n",
    "#(which will determine whether 2 rows are duplicated by checking all the column values)\n",
    "duplicated_apps = gplay.loc[gplay.duplicated()]\n",
    "\n",
    "print(gplay.duplicated())\n",
    "\n",
    "#print out number of rows that is duplicated \n",
    "#by default it will keep the first row as not duplicated i.e. subsequent rows with the exact same column values\n",
    "#will be treated as duplicated\n",
    "print(len(duplicated_apps))\n",
    "\n",
    "#this will return all the duplicated rows\n",
    "duplicated_apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use loc\n",
    "# duplicated_apps.loc[0]\n",
    "\n",
    "# iloc version, select the first_dup_app in duplicated_apps\n",
    "first_dup_app=duplicated_apps.iloc[0]\n",
    "\n",
    "first_dup_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Your Understanding Here '''\n",
    "#一开始我的理解是分别用iloc和loc来实现第二步选中，但是看来应该是分别用来实现两步？这里重复的“列”和\"行“确实把我搞晕了，行实际上指的是横坐标？第一步明明\n",
    "# 说了是检查duplicated rows（选中一行，即纵坐标的选中），为什么又说是查看gplay中重复的列？duplicated()返回的series中包含了所有重复行的名字，然后iloc\n",
    "#【0】即表示按顺序第一条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like Series, we can use the drop_duplicates() to remove the duplicated rows\n",
    "print(gplay.shape)\n",
    "print(gplay.drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请下载我们为你准备的 `Auto.csv` 文件，运用 `pandas` 库将其读入，并根据下面的要求进行相应的处理。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Auto.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，马力不是一个数字类型  \n",
    "主要原因是数据集中有一些缺失的值，用“?”字符表示\n",
    "\n",
    "还要注意，origin实际上是一个使用整数表示的分类变量，而不是实质上的数字变量  \n",
    "还要注意，年份既可以是时间的度量，也可以是描述汽车版本的分类变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q4. 阅读 `pd.read_csv()` 的文档，并将所有含有 `?` 的单元以 `Nan` 的形式读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Auto.csv\", na_values=\"?\")\n",
    "df[\"origin\"] = df[\"origin\"].astype(object)\n",
    "df[\"year\"] = df[\"year\"].astype(object)\n",
    "\n",
    "\n",
    "# Hint: You may also need to change the origin and year into object. You may need astype() function.\n",
    "# make sure you have replace all the ? cells on the **initial** data.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will get all the rows with at least a NaN value\n",
    "# axis = 1 means \"row-wise\"\n",
    "df.isna().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows with nan values\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q5. 将 `df` 中的所有含有 `Nan` 的行全部移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.isna().any(axis=1)]\n",
    "df\n",
    "# Hint: Actually one line could solve this problem. Think about ~ and .any() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习与深度学习的训练中，我们常常需要将一些损失或者其他的关键数据进行可视化，使得训练效果变得直观易于分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆️: 你可能没有上述的Python库，自己去安装它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上面我们处理过的 `df`，我们尝试着使用最小二乘法(OLS)来预测 `mpg` 和 `horsepower` 两者之间的联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能不能理解下面的代码在干什么，但是我们希望你去运行一下这段代码来对于线性回归有一个初步的了解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"horsepower\"]\n",
    "y = df[\"mpg\"]\n",
    "X = sm.add_constant(X)\n",
    "model1 = sm.OLS(y,X).fit()\n",
    "\n",
    "\n",
    "#model1 = sm.OLS(y,X.astype(float)).fit() \n",
    "\n",
    "\n",
    "#这段代码出现了报错ValueError: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).需要加上astype（）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q6. 了解 `matplotlib`，试对上述的代码的预测结果进行可视化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X['horsepower'], y, label='Actual')\n",
    "plt.plot(X['horsepower'], model1.predict(X), label='Predicted', color='red')\n",
    "plt.xlabel('Horsepower')\n",
    "plt.ylabel('MPG')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#我感觉这有些难，画出来的图像我没法弄成有效的\n",
    "# Hint: Consider about function scatter() and plot() in matplotlib.pyplot. \n",
    "# You may use model1.predict(X) to get the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 机器学习初探"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请下载我们为你准备的 `email_spam.csv` 文件，运用 `pandas` 库将其读入，并根据下面的要求进行相应的处理。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"email_spam.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q7. 现在我们将考虑对垃圾邮件进行分类。但在生成模型之前，首先将所有列数据转换为数字。具体地说:\n",
    "- `no` 修改为 `0`，`yes` 修改为 `1`\n",
    "- 为 `format` 和 `number` 使用虚拟变量编码(你可以使用 `pd.get_dummies()` 来生成虚拟变量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(value):\n",
    "    if value == 'no':\n",
    "        return 0\n",
    "    elif value == 'yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return value\n",
    "df = df.applymap(map_values)\n",
    "df = pd.get_dummies(df, columns=['format', 'number'], drop_first=True)\n",
    "X = df.iloc[:,1:]\n",
    "y = df[\"spam\"]\n",
    "# Hint: While you are using pd.get_dummies(), you may need to drop the previous column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q8. 使用 `train_test_split()` 将数据集划分为`70%`的训练集和`30%`的测试集(设置 `random_test=123` 以确保我们可以复制分割)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q9. 使用逻辑回归的模型来进行预测，同时看看你的准确率如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "#我没办法了解这些太高级的api的细节，只能从网上查找代码，但是我保证这些代码都从头到尾跑通过了，曾经处理过错误\n",
    "# Hint: While you are evaluating the model, you may need to use test data.\n",
    "# Think about the work you have done in the previous question. \n",
    "\n",
    "y_pred_score = accuracy_score(y_test,y_pred)\n",
    "y_pred_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Q10. 由于数据是不平衡的，最好生成混淆矩阵。了解什么是混淆矩阵以及如何在sklearn上实现它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' If you are interested in the confusion matrix, you may use the following code to get the confusion matrix. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to understand the code below.\n",
    "# It is just for plotting the confusion matrix.\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
