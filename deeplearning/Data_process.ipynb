{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸºæœ¬çš„æ•°æ®å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Series` å’Œ `Dataframe` æ˜¯ `pandas` åº“é‡Œé¢ä¸¤ä¸ªå¸¸è§çš„æ•°æ®ç±»å‹ï¼Œä¸ºäº†æ–¹ä¾¿ä½ ä»¬å¿«é€Ÿä¸Šæ‰‹ï¼Œæˆ‘ä»¬ä¼šå¯¹å®ƒä»¬è¿›è¡Œç›¸åº”çš„ä»‹ç»ä»¥åŠè¿›è¡Œä¸€äº›ç®€å•çš„ä»£ç å®æ“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ å¦‚æœä½ ä¸ä¼šä½¿ç”¨ `Jupyter Notebook`ï¼Œè¯·ç§»æ­¥è‡³ï¼š[Jupyter Notebookçš„ä½¿ç”¨](https://zhuanlan.zhihu.com/p/33105153)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed to ensure that we can replicate the numbers generated (for random function)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Series(np.random.randn(10))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use index to select specific values in s, have a try!\n",
    "# s[1:3],s[[1,2]],s[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seriesæ˜¯ä¸€ä¸ªå…·æœ‰è½´æ ‡ç­¾çš„1-D numpyæ•°ç»„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the index\n",
    "# Pandas data structure is extending numpy ndarray\n",
    "# but it comes with index(es)\n",
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index for the copy of s.\n",
    "s1 = s.copy()\n",
    "s1.index = [\"item 0\", \"item 1\", \"item 2\", \"item 3\", \"item 4\", \"item 5\", \"item 6\", \"item 7\", \"item 8\", \"item 9\"]\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reverse of s1\n",
    "s2 = Series(s.values[::-1], index=[\"item 0\", \"item 1\", \"item 2\", \"item 3\", \"item 4\", \"item 5\", \"item 6\", \"item 7\", \"item 8\", \"item 9\"])\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to perform s1 + s2\n",
    "s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice we are having same indexes\n",
    "s3 = pd.Series([\"d\", \"e\"])\n",
    "s4 = pd.Series([\"f\", \"g\"])\n",
    "s5 = pd.concat([s2, s3])\n",
    "s5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s5[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q1. æˆ‘ä»¬å¦‚ä½•å°† `s3` å’Œ `s4` å’Œåˆå¹¶åœ¨ä¸€èµ·ï¼ŒåŒæ—¶ä¿è¯ `index` æ˜¯é€’å¢çš„å‘¢ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s6=pd.concat([s3,s4],ignore_index=True)\n",
    "s6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame` æ˜¯ä¸€ä¸ªäºŒç»´è¡¨æ ¼æ•°æ®ã€‚`Series` å¯ç”¨çš„æ“ä½œä¹Ÿå¯ç”¨äº `Dataframe` (æˆ–ç±»ä¼¼çš„æ–¹å¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame([[1,2,3,4,5], [6,7,8,9,10]], columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯·ä¸‹è½½æˆ‘ä»¬ä¸ºä½ å‡†å¤‡çš„ `googleplaystore.csv` æ–‡ä»¶ï¼Œè¿ç”¨ `pandas` åº“å°†å…¶è¯»å…¥ï¼Œå¹¶æ ¹æ®ä¸‹é¢çš„è¦æ±‚è¿›è¡Œç›¸åº”çš„å¤„ç†ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplay = pd.read_csv(\"googleplaystore.csv\")\n",
    "gplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q2. è¯•å»é™¤ `gplay` ä¸­çš„ `Nan` æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplay.dropna(inplace=True)\n",
    "\n",
    "\n",
    "gplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q3. æŸ¥çœ‹`gplay`ä¸­é‡å¤çš„åˆ—ï¼Œè¯•ä½¿ç”¨`iloc`å’Œ`loc`æ¥é€‰ä¸­é‡å¤çš„è¡Œï¼Œå¹¶è°ˆè°ˆä»–ä»¬çš„åŒºåˆ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also possible to use the duplicated() method \n",
    "#(which will determine whether 2 rows are duplicated by checking all the column values)\n",
    "duplicated_apps = gplay.loc[gplay.duplicated()]\n",
    "\n",
    "print(gplay.duplicated())\n",
    "\n",
    "#print out number of rows that is duplicated \n",
    "#by default it will keep the first row as not duplicated i.e. subsequent rows with the exact same column values\n",
    "#will be treated as duplicated\n",
    "print(len(duplicated_apps))\n",
    "\n",
    "#this will return all the duplicated rows\n",
    "duplicated_apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use loc\n",
    "# duplicated_apps.loc[0]\n",
    "\n",
    "# iloc version, select the first_dup_app in duplicated_apps\n",
    "first_dup_app=duplicated_apps.iloc[0]\n",
    "\n",
    "first_dup_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Your Understanding Here '''\n",
    "#ä¸€å¼€å§‹æˆ‘çš„ç†è§£æ˜¯åˆ†åˆ«ç”¨ilocå’Œlocæ¥å®ç°ç¬¬äºŒæ­¥é€‰ä¸­ï¼Œä½†æ˜¯çœ‹æ¥åº”è¯¥æ˜¯åˆ†åˆ«ç”¨æ¥å®ç°ä¸¤æ­¥ï¼Ÿè¿™é‡Œé‡å¤çš„â€œåˆ—â€å’Œ\"è¡Œâ€œç¡®å®æŠŠæˆ‘ææ™•äº†ï¼Œè¡Œå®é™…ä¸ŠæŒ‡çš„æ˜¯æ¨ªåæ ‡ï¼Ÿç¬¬ä¸€æ­¥æ˜æ˜\n",
    "# è¯´äº†æ˜¯æ£€æŸ¥duplicated rowsï¼ˆé€‰ä¸­ä¸€è¡Œï¼Œå³çºµåæ ‡çš„é€‰ä¸­ï¼‰ï¼Œä¸ºä»€ä¹ˆåˆè¯´æ˜¯æŸ¥çœ‹gplayä¸­é‡å¤çš„åˆ—ï¼Ÿduplicated()è¿”å›çš„seriesä¸­åŒ…å«äº†æ‰€æœ‰é‡å¤è¡Œçš„åå­—ï¼Œç„¶åiloc\n",
    "#ã€0ã€‘å³è¡¨ç¤ºæŒ‰é¡ºåºç¬¬ä¸€æ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like Series, we can use the drop_duplicates() to remove the duplicated rows\n",
    "print(gplay.shape)\n",
    "print(gplay.drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è¯»å–æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯·ä¸‹è½½æˆ‘ä»¬ä¸ºä½ å‡†å¤‡çš„ `Auto.csv` æ–‡ä»¶ï¼Œè¿ç”¨ `pandas` åº“å°†å…¶è¯»å…¥ï¼Œå¹¶æ ¹æ®ä¸‹é¢çš„è¦æ±‚è¿›è¡Œç›¸åº”çš„å¤„ç†ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Auto.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼Œé©¬åŠ›ä¸æ˜¯ä¸€ä¸ªæ•°å­—ç±»å‹  \n",
    "ä¸»è¦åŸå› æ˜¯æ•°æ®é›†ä¸­æœ‰ä¸€äº›ç¼ºå¤±çš„å€¼ï¼Œç”¨â€œ?â€å­—ç¬¦è¡¨ç¤º\n",
    "\n",
    "è¿˜è¦æ³¨æ„ï¼Œoriginå®é™…ä¸Šæ˜¯ä¸€ä¸ªä½¿ç”¨æ•´æ•°è¡¨ç¤ºçš„åˆ†ç±»å˜é‡ï¼Œè€Œä¸æ˜¯å®è´¨ä¸Šçš„æ•°å­—å˜é‡  \n",
    "è¿˜è¦æ³¨æ„ï¼Œå¹´ä»½æ—¢å¯ä»¥æ˜¯æ—¶é—´çš„åº¦é‡ï¼Œä¹Ÿå¯ä»¥æ˜¯æè¿°æ±½è½¦ç‰ˆæœ¬çš„åˆ†ç±»å˜é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q4. é˜…è¯» `pd.read_csv()` çš„æ–‡æ¡£ï¼Œå¹¶å°†æ‰€æœ‰å«æœ‰ `?` çš„å•å…ƒä»¥ `Nan` çš„å½¢å¼è¯»å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Auto.csv\", na_values=\"?\")\n",
    "df[\"origin\"] = df[\"origin\"].astype(object)\n",
    "df[\"year\"] = df[\"year\"].astype(object)\n",
    "\n",
    "\n",
    "# Hint: You may also need to change the origin and year into object. You may need astype() function.\n",
    "# make sure you have replace all the ? cells on the **initial** data.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will get all the rows with at least a NaN value\n",
    "# axis = 1 means \"row-wise\"\n",
    "df.isna().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows with nan values\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q5. å°† `df` ä¸­çš„æ‰€æœ‰å«æœ‰ `Nan` çš„è¡Œå…¨éƒ¨ç§»é™¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.isna().any(axis=1)]\n",
    "df\n",
    "# Hint: Actually one line could solve this problem. Think about ~ and .any() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ•°æ®å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ çš„è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸éœ€è¦å°†ä¸€äº›æŸå¤±æˆ–è€…å…¶ä»–çš„å…³é”®æ•°æ®è¿›è¡Œå¯è§†åŒ–ï¼Œä½¿å¾—è®­ç»ƒæ•ˆæœå˜å¾—ç›´è§‚æ˜“äºåˆ†æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â¬†ï¸: ä½ å¯èƒ½æ²¡æœ‰ä¸Šè¿°çš„Pythonåº“ï¼Œè‡ªå·±å»å®‰è£…å®ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äºä¸Šé¢æˆ‘ä»¬å¤„ç†è¿‡çš„ `df`ï¼Œæˆ‘ä»¬å°è¯•ç€ä½¿ç”¨æœ€å°äºŒä¹˜æ³•(OLS)æ¥é¢„æµ‹ `mpg` å’Œ `horsepower` ä¸¤è€…ä¹‹é—´çš„è”ç³»ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯èƒ½ä¸èƒ½ç†è§£ä¸‹é¢çš„ä»£ç åœ¨å¹²ä»€ä¹ˆï¼Œä½†æ˜¯æˆ‘ä»¬å¸Œæœ›ä½ å»è¿è¡Œä¸€ä¸‹è¿™æ®µä»£ç æ¥å¯¹äºçº¿æ€§å›å½’æœ‰ä¸€ä¸ªåˆæ­¥çš„äº†è§£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"horsepower\"]\n",
    "y = df[\"mpg\"]\n",
    "X = sm.add_constant(X)\n",
    "model1 = sm.OLS(y,X).fit()\n",
    "\n",
    "\n",
    "#model1 = sm.OLS(y,X.astype(float)).fit() \n",
    "\n",
    "\n",
    "#è¿™æ®µä»£ç å‡ºç°äº†æŠ¥é”™ValueError: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).éœ€è¦åŠ ä¸Šastypeï¼ˆï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q6. äº†è§£ `matplotlib`ï¼Œè¯•å¯¹ä¸Šè¿°çš„ä»£ç çš„é¢„æµ‹ç»“æœè¿›è¡Œå¯è§†åŒ–å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X['horsepower'], y, label='Actual')\n",
    "plt.plot(X['horsepower'], model1.predict(X), label='Predicted', color='red')\n",
    "plt.xlabel('Horsepower')\n",
    "plt.ylabel('MPG')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#æˆ‘æ„Ÿè§‰è¿™æœ‰äº›éš¾ï¼Œç”»å‡ºæ¥çš„å›¾åƒæˆ‘æ²¡æ³•å¼„æˆæœ‰æ•ˆçš„\n",
    "# Hint: Consider about function scatter() and plot() in matplotlib.pyplot. \n",
    "# You may use model1.predict(X) to get the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æœºå™¨å­¦ä¹ åˆæ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯·ä¸‹è½½æˆ‘ä»¬ä¸ºä½ å‡†å¤‡çš„ `email_spam.csv` æ–‡ä»¶ï¼Œè¿ç”¨ `pandas` åº“å°†å…¶è¯»å…¥ï¼Œå¹¶æ ¹æ®ä¸‹é¢çš„è¦æ±‚è¿›è¡Œç›¸åº”çš„å¤„ç†ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"email_spam.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q7. ç°åœ¨æˆ‘ä»¬å°†è€ƒè™‘å¯¹åƒåœ¾é‚®ä»¶è¿›è¡Œåˆ†ç±»ã€‚ä½†åœ¨ç”Ÿæˆæ¨¡å‹ä¹‹å‰ï¼Œé¦–å…ˆå°†æ‰€æœ‰åˆ—æ•°æ®è½¬æ¢ä¸ºæ•°å­—ã€‚å…·ä½“åœ°è¯´:\n",
    "- `no` ä¿®æ”¹ä¸º `0`ï¼Œ`yes` ä¿®æ”¹ä¸º `1`\n",
    "- ä¸º `format` å’Œ `number` ä½¿ç”¨è™šæ‹Ÿå˜é‡ç¼–ç (ä½ å¯ä»¥ä½¿ç”¨ `pd.get_dummies()` æ¥ç”Ÿæˆè™šæ‹Ÿå˜é‡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(value):\n",
    "    if value == 'no':\n",
    "        return 0\n",
    "    elif value == 'yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return value\n",
    "df = df.applymap(map_values)\n",
    "df = pd.get_dummies(df, columns=['format', 'number'], drop_first=True)\n",
    "X = df.iloc[:,1:]\n",
    "y = df[\"spam\"]\n",
    "# Hint: While you are using pd.get_dummies(), you may need to drop the previous column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q8. ä½¿ç”¨ `train_test_split()` å°†æ•°æ®é›†åˆ’åˆ†ä¸º`70%`çš„è®­ç»ƒé›†å’Œ`30%`çš„æµ‹è¯•é›†(è®¾ç½® `random_test=123` ä»¥ç¡®ä¿æˆ‘ä»¬å¯ä»¥å¤åˆ¶åˆ†å‰²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q9. ä½¿ç”¨é€»è¾‘å›å½’çš„æ¨¡å‹æ¥è¿›è¡Œé¢„æµ‹ï¼ŒåŒæ—¶çœ‹çœ‹ä½ çš„å‡†ç¡®ç‡å¦‚ä½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "#æˆ‘æ²¡åŠæ³•äº†è§£è¿™äº›å¤ªé«˜çº§çš„apiçš„ç»†èŠ‚ï¼Œåªèƒ½ä»ç½‘ä¸ŠæŸ¥æ‰¾ä»£ç ï¼Œä½†æ˜¯æˆ‘ä¿è¯è¿™äº›ä»£ç éƒ½ä»å¤´åˆ°å°¾è·‘é€šè¿‡äº†ï¼Œæ›¾ç»å¤„ç†è¿‡é”™è¯¯\n",
    "# Hint: While you are evaluating the model, you may need to use test data.\n",
    "# Think about the work you have done in the previous question. \n",
    "\n",
    "y_pred_score = accuracy_score(y_test,y_pred)\n",
    "y_pred_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Q10. ç”±äºæ•°æ®æ˜¯ä¸å¹³è¡¡çš„ï¼Œæœ€å¥½ç”Ÿæˆæ··æ·†çŸ©é˜µã€‚äº†è§£ä»€ä¹ˆæ˜¯æ··æ·†çŸ©é˜µä»¥åŠå¦‚ä½•åœ¨sklearnä¸Šå®ç°å®ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' If you are interested in the confusion matrix, you may use the following code to get the confusion matrix. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to understand the code below.\n",
    "# It is just for plotting the confusion matrix.\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
